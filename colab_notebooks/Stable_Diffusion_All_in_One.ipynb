{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JMQuehl/stable-diffusion/blob/colab/colab_notebooks/Stable_Diffusion_All_in_One.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gd-vX3cavOCt"
      },
      "source": [
        "# **Stable Diffusion** \n",
        "In order to use this colab, first get an access token for the stable diffusion weights hosted on huggingface (https://huggingface.co/CompVis/stable-diffusion). If you accepted the license for this model and generated a key, put that key into the access_token field below.\n",
        "\n",
        "If you get cuda_out_of_memory or similar errors, try using the lower precision model. There is an (optional) SFW filter, which you can enable below in order to ensure that no offensive content might be created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDUi8tYoUEWZ"
      },
      "outputs": [],
      "source": [
        "access_token = \"put_your_token_here\" #@param{type: 'string'}\n",
        "use_lower_precision = False #@param{type: 'boolean'}\n",
        "use_pipeline_with_safety_filter = False #@param{type: 'boolean'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYOlvQ1nQL7c"
      },
      "source": [
        "# 1. Setup\n",
        "\n",
        "First, please make sure you are using a GPU runtime to run this notebook, so inference is much faster. If the following command fails, use the `Runtime` menu above and select `Change runtime type`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHkHsdtnry57"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paJt_cx5QgVz"
      },
      "source": [
        "Prepare and set up libraries for Stable Diffusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIrgth7sqFML"
      },
      "outputs": [],
      "source": [
        "!pip install diffusers==0.2.4\n",
        "!pip install transformers scipy ftfy\n",
        "!pip install \"ipywidgets>=7,<8\"\n",
        "!wget https://raw.githubusercontent.com/huggingface/diffusers/4674fdf807cdefd4db1758067c0207872d805f8c/examples/inference/image_to_image.py\n",
        "\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buFaskR-mqqL"
      },
      "source": [
        "Prepare Stable Diffusion Pipelines for Text2Image and Image2Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSKWBKFPArKS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gc\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from image_to_image import StableDiffusionImg2ImgPipeline, preprocess\n",
        "\n",
        "if use_lower_precision: \n",
        "    pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", revision=\"fp16\", torch_dtype=torch.float16, use_auth_token=access_token) \n",
        "    im2im_pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", revision=\"fp16\", torch_dtype=torch.float16, use_auth_token=access_token)\n",
        "else:\n",
        "    pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", use_auth_token=access_token)\n",
        "    im2im_pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", revision=\"fp16\", torch_dtype=torch.float16, use_auth_token=access_token)\n",
        "\n",
        "\n",
        "if not use_pipeline_with_safety_filter:\n",
        "  def dummy_checker(images, **kwargs): return images, False\n",
        "  pipe.safety_checker = dummy_checker\n",
        "  im2im_pipe.safety_checker = dummy_checker\n",
        "\n",
        "pipe = pipe.to(\"cuda\")\n",
        "im2im_pipe = im2im_pipe.to(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPT8PCT67TvA"
      },
      "source": [
        "Define Helper Function(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2YMlltn7V6J"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "def image_grid(imgs, rows, cols):\n",
        "    assert len(imgs) == rows*cols\n",
        "\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
        "    grid_w, grid_h = grid.size\n",
        "    \n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
        "    return grid\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NXFFfjsS-zj"
      },
      "source": [
        "# 2. Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xQFDu7YcYH9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "import shutil\n",
        "#@markdown #####**General-Settings:**\n",
        "prompt = \" Amazing, complex, intricate and highly detailed treehouse in a snow covered bonsai tree on top of a table, steampunk, vibrant colors, vibrant, beautiful, contrast, neon highlights, Highly detailed, ray tracing, digital painting, artstation, concept art, smooth, sharp focus, illustration, art by Beeple, Mike Winklemann, 8 k\" #@param{type: 'string'}\n",
        "width = 512 #@param{type: 'number'}\n",
        "height = 512 #@param{type: 'number'}\n",
        "steps = 50 #@param{type: 'number'}\n",
        "use_seed = False #@param{type: 'boolean'}\n",
        "seed = 1240415754 #@param{type: 'number'}\n",
        "guidance_scale = 7.5 #@param{type: 'number'}\n",
        "num_cols = 2 #@param{type: 'number'}\n",
        "num_rows = 2 #@param{type: 'number'}\n",
        "batch_size = num_rows * num_cols\n",
        "\n",
        "#@markdown #####**Init_Image-Settings:**\n",
        "use_init_image = False #@param{type: 'boolean'}\n",
        "init_image_strength = 0.6 #@param{type: 'number'}\n",
        "upload_new_image = True #@param{type: 'boolean'}\n",
        "#@markdown if a new image is uploaded that takes precedence over the batch image.\n",
        "use_image_from_previous_batch_as_init = True #@param{type: 'boolean'}\n",
        "batch_image_number = 0 #@param{type: 'number'}\n",
        "resize = False #@param{type: 'boolean'}\n",
        "\n",
        "if height > 512 or width > 512:\n",
        "  print(\"Warning: If height or width are larger than 512 coherence might be lost!\")\n",
        "\n",
        "generator = torch.Generator(\"cuda\")\n",
        "\n",
        "if use_init_image :\n",
        "  if use_image_from_previous_batch_as_init and \"all_images\" in globals():\n",
        "    init_image = all_images[batch_image_number]\n",
        "\n",
        "  if not \"init_image\" in globals() or upload_new_image: \n",
        "    upload_folder = 'upload'\n",
        "    if os.path.isdir(upload_folder):\n",
        "        shutil.rmtree(upload_folder)\n",
        "    os.mkdir(upload_folder)\n",
        "    # upload images\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    for filename in uploaded.keys() :\n",
        "      dst_path = os.path.join(upload_folder, filename)\n",
        "      print(f'move {filename} to {dst_path}')\n",
        "      shutil.move(filename, dst_path)\n",
        "    filename = os.path.join(upload_folder, list(uploaded.keys())[0])\n",
        "    init_image = Image.open(filename).convert(\"RGB\")\n",
        "\n",
        "\n",
        "  if (resize):\n",
        "    init_image = init_image.resize((width, height))\n",
        "  init_image_cuda = preprocess(init_image)\n",
        "  print (\"Init image will be: \")\n",
        "  display(init_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxpIxszve_wY"
      },
      "source": [
        "# 3. Run Standard Pipeline\n",
        "\n",
        "If using a seed this image will always be the same for the same *prompt*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEErJFjlrSWS"
      },
      "outputs": [],
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "from torch import autocast\n",
        "\n",
        "if not use_seed :\n",
        "  import random\n",
        "  random.seed()\n",
        "  seed = random.randint(0, 2**32)\n",
        "\n",
        "generator = torch.Generator(\"cuda\").manual_seed(seed)\n",
        "print(\"using seed: {seed}\".format(seed = seed))\n",
        "\n",
        "prompts = [prompt] * num_cols\n",
        "\n",
        "all_images = []\n",
        "for i in range(num_rows):\n",
        "  with autocast(\"cuda\"):\n",
        "    if not use_init_image or \"init_image_cuda\" not in globals():\n",
        "      images = pipe(prompts, num_inference_steps=steps, height=height, width=width, generator=generator, guidance_scale=guidance_scale)[\"sample\"]  # image here is in [PIL format](https://pillow.readthedocs.io/en/stable/)\n",
        "    else:\n",
        "      images = pipe(prompts, num_inference_steps=steps, generator=generator, guidance_scale=guidance_scale, init_image=init_image_cuda, strength=init_image_strength)[\"sample\"]\n",
        "  all_images.extend(images)\n",
        "\n",
        "grid = image_grid(all_images, rows=num_rows, cols=num_cols)\n",
        "grid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExMl2s8jTsX-"
      },
      "source": [
        "Set Upscaler Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bHr_fQVHkkP"
      },
      "outputs": [],
      "source": [
        "image_to_enhance = 1 #@param{type: 'number'}\n",
        "pil_img = all_images[image_to_enhance]\n",
        "\n",
        "esrgan_model_name = \"RealESRGAN_x4plus\" #@param [\"RealESRGAN_x4plus\", \"RealESRNet_x4plus\", \"RealESRGAN_x4plus_anime_6B\", \"RealESRGAN_x2plus\", \"realesr-animevideov3\"]\n",
        "outscale = 3.5 #@param{type: 'number'}\n",
        "face_enhance = True #@param{type: 'boolean'}\n",
        "all_images[image_to_enhance]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LZq_q3XIe53"
      },
      "source": [
        "# 4. Prepare ESRGAN for upscaling an image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lv1dIt_zImeP"
      },
      "outputs": [],
      "source": [
        "# Clone Real-ESRGAN and enter the Real-ESRGAN\n",
        "!git clone https://github.com/xinntao/Real-ESRGAN.git\n",
        "%cd Real-ESRGAN\n",
        "# Set up the environment\n",
        "!pip install basicsr\n",
        "!pip install facexlib\n",
        "!pip install gfpgan\n",
        "!pip install -r requirements.txt\n",
        "!python setup.py develop\n",
        "# Download the pre-trained model\n",
        "!wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth -P experiments/pretrained_models\n",
        "!wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.1/RealESRNet_x4plus.pth -P experiments/pretrained_models\n",
        "!wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/RealESRGAN_x4plus_anime_6B.pth -P experiments/pretrained_models\n",
        "!wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.1/RealESRGAN_x2plus.pth -P experiments/pretrained_models\n",
        "!wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesr-animevideov3.pth -P experiments/pretrained_models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2DjRfyrJILm"
      },
      "source": [
        "Define ESRGAN-Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLcz1AJdJHdX"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import glob\n",
        "import os\n",
        "import numpy\n",
        "from basicsr.archs.rrdbnet_arch import RRDBNet\n",
        "\n",
        "from realesrgan import RealESRGANer\n",
        "from realesrgan.archs.srvgg_arch import SRVGGNetCompact\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "tile = 0\n",
        "tile_pad = 10\n",
        "pre_pad = 0\n",
        "gpu_id = None\n",
        "fp32 = True\n",
        "alpha_upsampler = \"realesrgan\" # realesrgan | bicubic\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# determine models according to model names\n",
        "if esrgan_model_name in ['RealESRGAN_x4plus', 'RealESRNet_x4plus']:  # x4 RRDBNet model\n",
        "    model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=4)\n",
        "    netscale = 4\n",
        "elif esrgan_model_name in ['RealESRGAN_x4plus_anime_6B']:  # x4 RRDBNet model with 6 blocks\n",
        "    model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=6, num_grow_ch=32, scale=4)\n",
        "    netscale = 4\n",
        "elif esrgan_model_name in ['RealESRGAN_x2plus']:  # x2 RRDBNet model\n",
        "    model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=2)\n",
        "    netscale = 2\n",
        "elif esrgan_model_name in ['realesr-animevideov3']:  # x4 VGG-style model (XS size)\n",
        "    model = SRVGGNetCompact(num_in_ch=3, num_out_ch=3, num_feat=64, num_conv=16, upscale=4, act_type='prelu')\n",
        "    netscale = 4\n",
        "\n",
        "# determine model paths\n",
        "model_path = os.path.join('experiments/pretrained_models', esrgan_model_name + '.pth')\n",
        "if not os.path.isfile(model_path):\n",
        "    model_path = os.path.join('realesrgan/weights', esrgan_model_name + '.pth')\n",
        "if not os.path.isfile(model_path):\n",
        "    raise ValueError(f'Model {esrgan_model_name} does not exist.')\n",
        "\n",
        "# restorer\n",
        "upsampler = RealESRGANer(\n",
        "    scale=netscale,\n",
        "    model_path=model_path,\n",
        "    model=model,\n",
        "    tile=tile,\n",
        "    tile_pad=tile_pad,\n",
        "    pre_pad=pre_pad,\n",
        "    half=not fp32,\n",
        "    gpu_id=gpu_id)\n",
        "\n",
        "if face_enhance:  # Use GFPGAN for face enhancement\n",
        "    from gfpgan import GFPGANer\n",
        "    face_enhancer = GFPGANer(\n",
        "        model_path='https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth',\n",
        "        upscale=outscale,\n",
        "        arch='clean',\n",
        "        channel_multiplier=2,\n",
        "        bg_upsampler=upsampler)\n",
        "\n",
        "img = numpy.array(pil_img)\n",
        "img = img[:, :, ::-1].copy()\n",
        "\n",
        "if len(img.shape) == 3 and img.shape[2] == 4:\n",
        "    img_mode = 'RGBA'\n",
        "else:\n",
        "    img_mode = None\n",
        "\n",
        "try:\n",
        "    if face_enhance:\n",
        "        _, _, output = face_enhancer.enhance(img, has_aligned=False, only_center_face=False, paste_back=True)\n",
        "    else:\n",
        "        output, _ = upsampler.enhance(img, outscale=args.outscale)\n",
        "except RuntimeError as error:\n",
        "    print('Error', error)\n",
        "    print('If you encounter CUDA out of memory, try to set --tile with a smaller number.')\n",
        "\n",
        "cv2_imshow(output)\n",
        "#plt.imshow(cv2.cvtColor(output, cv2.COLOR_BGR2RGB))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnb-nd9_TZhm"
      },
      "source": [
        "# 5. Run Upscaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6n6QKGwTXhb"
      },
      "outputs": [],
      "source": [
        "img = numpy.array(pil_img)\n",
        "img = img[:, :, ::-1].copy()\n",
        "\n",
        "if len(img.shape) == 3 and img.shape[2] == 4:\n",
        "    img_mode = 'RGBA'\n",
        "else:\n",
        "    img_mode = None\n",
        "\n",
        "try:\n",
        "    if face_enhance:\n",
        "        _, _, output = face_enhancer.enhance(img, has_aligned=False, only_center_face=False, paste_back=True)\n",
        "    else:\n",
        "        output, _ = upsampler.enhance(img, outscale=args.outscale)\n",
        "except RuntimeError as error:\n",
        "    print('Error', error)\n",
        "    print('If you encounter CUDA out of memory, try to set --tile with a smaller number.')\n",
        "\n",
        "cv2_imshow(output)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "nnb-nd9_TZhm"
      ],
      "machine_shape": "hm",
      "name": "Stable Diffusion - All in One",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}